{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# üèüÔ∏è Soccer Banner Segmentation - EDA & Data Preparation\n",
                "\n",
                "Questo notebook prepara i dati per il training del modello YOLOv11:\n",
                "- Download dataset da Kaggle e Roboflow\n",
                "- Preprocessing e split (70/20/10 per Kaggle)\n",
                "- Analisi esplorativa comparativa dei dataset\n",
                "\n",
                "---"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## üì¶ 1. Installazione Dipendenze\n",
                "\n",
                "Decommentare ed eseguire questa cella solo al primo avvio."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# ============================================================================\n",
                "# INSTALLAZIONE DIPENDENZE (eseguire solo se necessario)\n",
                "# ============================================================================\n",
                "# pip install \"numpy<2.0\" ultralytics opencv-python roboflow kaggle pyyaml matplotlib tqdm python-dotenv seaborn"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## üìö 2. Import delle Librerie"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# ============================================================================\n",
                "# IMPORT LIBRERIE\n",
                "# ============================================================================\n",
                "\n",
                "# Librerie Standard Python\n",
                "import sys\n",
                "import os\n",
                "import yaml\n",
                "import shutil\n",
                "import glob\n",
                "import random\n",
                "\n",
                "# Librerie Scientifiche\n",
                "import numpy as np\n",
                "import pandas as pd\n",
                "import cv2\n",
                "\n",
                "# Visualizzazione\n",
                "import matplotlib.pyplot as plt\n",
                "import seaborn as sns\n",
                "\n",
                "# Deep Learning\n",
                "import torch\n",
                "\n",
                "# Dataset & Utilities\n",
                "from roboflow import Roboflow\n",
                "from dotenv import load_dotenv\n",
                "from tqdm import tqdm\n",
                "from pathlib import Path\n",
                "\n",
                "# Caricamento variabili d'ambiente\n",
                "load_dotenv()\n",
                "\n",
                "# Configurazione visualizzazione\n",
                "plt.style.use('seaborn-v0_8-whitegrid')\n",
                "sns.set_palette('husl')\n",
                "\n",
                "print(\"‚úÖ Ambiente configurato correttamente.\")\n",
                "print(f\"   PyTorch version: {torch.__version__}\")\n",
                "print(f\"   CUDA disponibile: {torch.cuda.is_available()}\")\n",
                "if torch.cuda.is_available():\n",
                "    print(f\"   GPU: {torch.cuda.get_device_name(0)}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## üìÅ 3. Configurazione Directory di Lavoro\n",
                "\n",
                "Creazione struttura cartelle: input, output, dataset, model."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# ============================================================================\n",
                "# CONFIGURAZIONE DIRECTORY\n",
                "# ============================================================================\n",
                "# Struttura del progetto:\n",
                "#   BASE_DIR/\n",
                "#   ‚îú‚îÄ‚îÄ input/           # Video e immagini di input per inferenza\n",
                "#   ‚îú‚îÄ‚îÄ output/          # Risultati del training e inferenza\n",
                "#   ‚îú‚îÄ‚îÄ dataset/         # Dataset scaricati\n",
                "#   ‚îÇ   ‚îú‚îÄ‚îÄ kaggle_raw/\n",
                "#   ‚îÇ   ‚îú‚îÄ‚îÄ kaggle_dataset/\n",
                "#   ‚îÇ   ‚îî‚îÄ‚îÄ roboflow_dataset/\n",
                "#   ‚îî‚îÄ‚îÄ model/           # Modelli addestrati\n",
                "# ============================================================================\n",
                "\n",
                "# Directory root del progetto\n",
                "BASE_DIR = Path(os.getcwd())\n",
                "\n",
                "# Sottodirectory principali\n",
                "INPUT_DIR = BASE_DIR / \"input\"\n",
                "OUTPUT_DIR = BASE_DIR / \"output\"\n",
                "DATASETS_DIR = BASE_DIR / \"dataset\"\n",
                "MODEL_DIR = BASE_DIR / \"model\"\n",
                "\n",
                "# Creazione automatica delle directory\n",
                "for directory in [INPUT_DIR, OUTPUT_DIR, DATASETS_DIR, MODEL_DIR]:\n",
                "    directory.mkdir(parents=True, exist_ok=True)\n",
                "\n",
                "print(\"‚úÖ Directory configurate correttamente:\")\n",
                "print(f\"   üìÇ Base:     {BASE_DIR}\")\n",
                "print(f\"   üìÇ Input:    {INPUT_DIR}\")\n",
                "print(f\"   üìÇ Output:   {OUTPUT_DIR}\")\n",
                "print(f\"   üìÇ Dataset:  {DATASETS_DIR}\")\n",
                "print(f\"   üìÇ Model:    {MODEL_DIR}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## üîê 4. Validazione Credenziali API"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# ============================================================================\n",
                "# VALIDAZIONE CREDENZIALI API\n",
                "# ============================================================================\n",
                "\n",
                "KAGGLE_USERNAME = os.getenv('KAGGLE_USERNAME')\n",
                "KAGGLE_KEY = os.getenv('KAGGLE_KEY')\n",
                "ROBOFLOW_KEY = os.getenv('ROBOFLOW_KEY')\n",
                "\n",
                "missing_keys = []\n",
                "if not KAGGLE_USERNAME:\n",
                "    missing_keys.append('KAGGLE_USERNAME')\n",
                "if not KAGGLE_KEY:\n",
                "    missing_keys.append('KAGGLE_KEY')\n",
                "if not ROBOFLOW_KEY:\n",
                "    missing_keys.append('ROBOFLOW_KEY')\n",
                "\n",
                "if missing_keys:\n",
                "    raise RuntimeError(\n",
                "        f\"‚ùå Chiavi mancanti nel file .env: {', '.join(missing_keys)}\\n\"\n",
                "        f\"   Crea un file .env con:\\n\"\n",
                "        f\"   KAGGLE_USERNAME=tuo_username\\n\"\n",
                "        f\"   KAGGLE_KEY=tua_chiave\\n\"\n",
                "        f\"   ROBOFLOW_KEY=tua_chiave\"\n",
                "    )\n",
                "\n",
                "os.environ['KAGGLE_USERNAME'] = str(KAGGLE_USERNAME)\n",
                "os.environ['KAGGLE_KEY'] = str(KAGGLE_KEY)\n",
                "\n",
                "print(\"‚úÖ Credenziali caricate e validate.\")\n",
                "print(f\"   üë§ Kaggle User: {KAGGLE_USERNAME}\")\n",
                "print(f\"   üîë Roboflow Key: {str(ROBOFLOW_KEY)[:8]}...\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## üì• 5. Download Dataset Kaggle"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# ============================================================================\n",
                "# ACQUISIZIONE DATASET KAGGLE\n",
                "# ============================================================================\n",
                "\n",
                "def acquire_kaggle_dataset():\n",
                "    \"\"\"Scarica e prepara il dataset Kaggle.\"\"\"\n",
                "    print(\"üì• Download Dataset Kaggle...\")\n",
                "    \n",
                "    kaggle_cmd = \"kaggle datasets download -d swagatajana/football-match-adboards-mask-dataset --force\"\n",
                "    exit_code = os.system(kaggle_cmd)\n",
                "    \n",
                "    if exit_code != 0:\n",
                "        print(\"‚ö†Ô∏è  Download fallito. Verifica credenziali Kaggle.\")\n",
                "        return None\n",
                "    \n",
                "    zip_path = \"football-match-adboards-mask-dataset.zip\"\n",
                "    kaggle_raw_dir = DATASETS_DIR / \"kaggle_raw\"\n",
                "    \n",
                "    if os.path.exists(zip_path):\n",
                "        print(\"   üì¶ Estrazione archivio...\")\n",
                "        shutil.unpack_archive(zip_path, kaggle_raw_dir)\n",
                "        os.remove(zip_path)\n",
                "        print(f\"   ‚úÖ Dataset estratto: {kaggle_raw_dir}\")\n",
                "        return kaggle_raw_dir\n",
                "    \n",
                "    return None\n",
                "\n",
                "KAGGLE_DIR = acquire_kaggle_dataset()\n",
                "print(\"=\"*60)"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## üì• 6. Download Dataset Roboflow"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# ============================================================================\n",
                "# ACQUISIZIONE DATASET ROBOFLOW\n",
                "# ============================================================================\n",
                "\n",
                "def acquire_roboflow_dataset():\n",
                "    \"\"\"Scarica dataset Roboflow con split pre-configurato.\"\"\"\n",
                "    print(\"\\nüì• Download Dataset Roboflow v4...\")\n",
                "    \n",
                "    rf = Roboflow(api_key=ROBOFLOW_KEY)\n",
                "    project = rf.workspace(\"workspace-tp24t\").project(\"soccer-banner-segmentation\")\n",
                "    roboflow_dir = DATASETS_DIR / \"roboflow_dataset\"\n",
                "    \n",
                "    dataset = project.version(4).download(\n",
                "        model_format=\"yolov11\",\n",
                "        location=str(roboflow_dir)\n",
                "    )\n",
                "    \n",
                "    yaml_path = str(roboflow_dir / 'data.yaml')\n",
                "    \n",
                "    # Verifica split\n",
                "    train_imgs = len(list((roboflow_dir / 'train' / 'images').glob('*.*'))) if (roboflow_dir / 'train' / 'images').exists() else 0\n",
                "    valid_imgs = len(list((roboflow_dir / 'valid' / 'images').glob('*.*'))) if (roboflow_dir / 'valid' / 'images').exists() else 0\n",
                "    test_imgs = len(list((roboflow_dir / 'test' / 'images').glob('*.*'))) if (roboflow_dir / 'test' / 'images').exists() else 0\n",
                "    total = train_imgs + valid_imgs + test_imgs\n",
                "    \n",
                "    print(f\"   ‚úÖ Dataset scaricato: {roboflow_dir}\")\n",
                "    print(f\"   üìä Split (gi√† effettuato da Roboflow):\")\n",
                "    print(f\"      Train: {train_imgs:3d} immagini ({train_imgs/total*100:.0f}%)\" if total > 0 else \"      Train: 0\")\n",
                "    print(f\"      Valid: {valid_imgs:3d} immagini ({valid_imgs/total*100:.0f}%)\" if total > 0 else \"      Valid: 0\")\n",
                "    print(f\"      Test:  {test_imgs:3d} immagini ({test_imgs/total*100:.0f}%)\" if total > 0 else \"      Test:  0\")\n",
                "    \n",
                "    return yaml_path\n",
                "\n",
                "ROBO_YAML_PATH = acquire_roboflow_dataset()\n",
                "print(\"=\"*60)"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## üè∑Ô∏è 7. Preprocessing Dataset Kaggle (Split 70/20/10)\n",
                "\n",
                "Conversione maschere ‚Üí annotazioni YOLO con split train/valid/test."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# ============================================================================\n",
                "# PREPROCESSING DATASET KAGGLE CON SPLIT 70/20/10\n",
                "# ============================================================================\n",
                "\n",
                "def prepare_kaggle_labels():\n",
                "    \"\"\"\n",
                "    Converte maschere binarie in label YOLO con split 70/20/10.\n",
                "    \"\"\"\n",
                "    # Directory sorgente\n",
                "    raw_base = DATASETS_DIR / \"kaggle_raw\"\n",
                "    src_img_dir = raw_base / 'Tagged_Images' / 'Tagged Images'\n",
                "    src_msk_dir = raw_base / 'Masks' / 'Masks'\n",
                "    \n",
                "    # Directory destinazione\n",
                "    dest_base = DATASETS_DIR / 'kaggle_dataset'\n",
                "    \n",
                "    # Creazione struttura split\n",
                "    splits = ['train', 'valid', 'test']\n",
                "    for split in splits:\n",
                "        (dest_base / split / 'images').mkdir(parents=True, exist_ok=True)\n",
                "        (dest_base / split / 'labels').mkdir(parents=True, exist_ok=True)\n",
                "    \n",
                "    # Ricerca file sorgente\n",
                "    img_files = sorted(glob.glob(str(src_img_dir / \"*.jpg\")))\n",
                "    msk_files = sorted(glob.glob(str(src_msk_dir / \"*.png\")))\n",
                "    num_pairs = min(len(img_files), len(msk_files))\n",
                "    \n",
                "    print(f\"üîÑ Elaborazione di {num_pairs} coppie immagine-maschera...\")\n",
                "    \n",
                "    # Shuffle per split randomico\n",
                "    indices = list(range(num_pairs))\n",
                "    random.seed(42)\n",
                "    random.shuffle(indices)\n",
                "    \n",
                "    # Calcolo indici split 70/20/10\n",
                "    train_end = int(0.70 * num_pairs)\n",
                "    valid_end = int(0.90 * num_pairs)\n",
                "    \n",
                "    split_indices = {\n",
                "        'train': indices[:train_end],\n",
                "        'valid': indices[train_end:valid_end],\n",
                "        'test': indices[valid_end:]\n",
                "    }\n",
                "    \n",
                "    counts = {'train': 0, 'valid': 0, 'test': 0}\n",
                "    \n",
                "    for split, idx_list in split_indices.items():\n",
                "        for i in tqdm(idx_list, desc=f\"Processing {split}\"):\n",
                "            new_name = f\"kaggle_{i:04d}\"\n",
                "            \n",
                "            # Copia immagine\n",
                "            shutil.copy(img_files[i], dest_base / split / 'images' / f\"{new_name}.jpg\")\n",
                "            \n",
                "            # Conversione maschera\n",
                "            mask = cv2.imread(msk_files[i], cv2.IMREAD_GRAYSCALE)\n",
                "            if mask is None:\n",
                "                continue\n",
                "            \n",
                "            _, binary = cv2.threshold(mask, 127, 255, cv2.THRESH_BINARY)\n",
                "            contours, _ = cv2.findContours(binary, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n",
                "            \n",
                "            h, w = mask.shape\n",
                "            polygons = []\n",
                "            \n",
                "            for contour in contours:\n",
                "                if len(contour) < 3:\n",
                "                    continue\n",
                "                points = contour.reshape(-1, 2).astype(np.float32)\n",
                "                points[:, 0] /= w\n",
                "                points[:, 1] /= h\n",
                "                coords_str = \" \".join([f\"{pt[0]:.6f} {pt[1]:.6f}\" for pt in points])\n",
                "                polygons.append(f\"0 {coords_str}\")\n",
                "            \n",
                "            with open(dest_base / split / 'labels' / f\"{new_name}.txt\", \"w\") as f:\n",
                "                f.write(\"\\n\".join(polygons))\n",
                "            \n",
                "            counts[split] += 1\n",
                "    \n",
                "    # Generazione YAML\n",
                "    kaggle_config = {\n",
                "        'path': str(dest_base.absolute()),\n",
                "        'train': 'train/images',\n",
                "        'val': 'valid/images',\n",
                "        'test': 'test/images',\n",
                "        'nc': 1,\n",
                "        'names': ['banner']\n",
                "    }\n",
                "    \n",
                "    yaml_path = dest_base / 'kaggle_data.yaml'\n",
                "    with open(yaml_path, 'w') as f:\n",
                "        yaml.dump(kaggle_config, f, default_flow_style=False)\n",
                "    \n",
                "    total = sum(counts.values())\n",
                "    print(f\"\\n‚úÖ Preprocessing completato con split 70/20/10:\")\n",
                "    print(f\"   üìä Train: {counts['train']} ({counts['train']/total*100:.0f}%)\")\n",
                "    print(f\"   üìä Valid: {counts['valid']} ({counts['valid']/total*100:.0f}%)\")\n",
                "    print(f\"   üìä Test:  {counts['test']} ({counts['test']/total*100:.0f}%)\")\n",
                "    print(f\"   üìÑ Config: {yaml_path}\")\n",
                "    \n",
                "    return str(yaml_path)\n",
                "\n",
                "KAG_YAML_PATH = prepare_kaggle_labels()"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## üîß 8. Fix Configurazione Roboflow"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# ============================================================================\n",
                "# FIX CONFIGURAZIONE YAML ROBOFLOW\n",
                "# ============================================================================\n",
                "\n",
                "def fix_roboflow_yaml(yaml_path: str) -> bool:\n",
                "    \"\"\"Corregge path nel data.yaml Roboflow.\"\"\"\n",
                "    \n",
                "    if not os.path.exists(yaml_path):\n",
                "        print(f\"‚ùå File non trovato: {yaml_path}\")\n",
                "        return False\n",
                "    \n",
                "    with open(yaml_path, 'r') as f:\n",
                "        config = yaml.safe_load(f)\n",
                "    \n",
                "    base_dir = Path(yaml_path).parent.absolute()\n",
                "    \n",
                "    config['path'] = str(base_dir)\n",
                "    config['train'] = str(base_dir / 'train' / 'images')\n",
                "    config['val'] = str(base_dir / 'valid' / 'images')\n",
                "    config['nc'] = 1\n",
                "    config['names'] = {0: 'banner'}\n",
                "    \n",
                "    # Validazione\n",
                "    train_path = Path(config['train'])\n",
                "    val_path = Path(config['val'])\n",
                "    \n",
                "    if not train_path.exists() or not val_path.exists():\n",
                "        print(\"‚ùå Directory train/ o valid/ non trovate\")\n",
                "        return False\n",
                "    \n",
                "    n_train = len(list(train_path.glob('*.*')))\n",
                "    n_val = len(list(val_path.glob('*.*')))\n",
                "    \n",
                "    with open(yaml_path, 'w') as f:\n",
                "        yaml.dump(config, f, default_flow_style=False)\n",
                "    \n",
                "    print(f\"‚úÖ data.yaml Roboflow corretto\")\n",
                "    print(f\"   üìä Train: {n_train} immagini\")\n",
                "    print(f\"   üìä Valid: {n_val} immagini\")\n",
                "    \n",
                "    return True\n",
                "\n",
                "fix_roboflow_yaml(ROBO_YAML_PATH)"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "---\n",
                "# üìä Exploratory Data Analysis (EDA)\n",
                "\n",
                "Analisi comparativa dei dataset Kaggle e Roboflow.\n",
                "\n",
                "---"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## üìà 9. Statistiche Dataset"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# ============================================================================\n",
                "# STATISTICHE GENERALI DEI DATASET\n",
                "# ============================================================================\n",
                "\n",
                "def get_dataset_stats(dataset_path: Path, name: str):\n",
                "    \"\"\"Calcola statistiche per un dataset.\"\"\"\n",
                "    stats = {'name': name, 'splits': {}}\n",
                "    \n",
                "    for split in ['train', 'valid', 'test']:\n",
                "        img_dir = dataset_path / split / 'images'\n",
                "        lbl_dir = dataset_path / split / 'labels'\n",
                "        \n",
                "        if not img_dir.exists():\n",
                "            continue\n",
                "            \n",
                "        images = list(img_dir.glob('*.*'))\n",
                "        labels = list(lbl_dir.glob('*.txt')) if lbl_dir.exists() else []\n",
                "        \n",
                "        # Dimensioni immagini\n",
                "        sizes = []\n",
                "        for img_path in images[:50]:  # Campione di 50\n",
                "            img = cv2.imread(str(img_path))\n",
                "            if img is not None:\n",
                "                sizes.append((img.shape[1], img.shape[0]))\n",
                "        \n",
                "        stats['splits'][split] = {\n",
                "            'n_images': len(images),\n",
                "            'n_labels': len(labels),\n",
                "            'avg_width': np.mean([s[0] for s in sizes]) if sizes else 0,\n",
                "            'avg_height': np.mean([s[1] for s in sizes]) if sizes else 0\n",
                "        }\n",
                "    \n",
                "    return stats\n",
                "\n",
                "# Calcolo statistiche\n",
                "kaggle_stats = get_dataset_stats(DATASETS_DIR / 'kaggle_dataset', 'Kaggle')\n",
                "roboflow_stats = get_dataset_stats(DATASETS_DIR / 'roboflow_dataset', 'Roboflow')\n",
                "\n",
                "# Visualizzazione\n",
                "print(\"üìä STATISTICHE DATASET\")\n",
                "print(\"=\"*60)\n",
                "\n",
                "for stats in [kaggle_stats, roboflow_stats]:\n",
                "    print(f\"\\nüóÇÔ∏è  {stats['name']}:\")\n",
                "    total = sum(s['n_images'] for s in stats['splits'].values())\n",
                "    for split, data in stats['splits'].items():\n",
                "        pct = data['n_images']/total*100 if total > 0 else 0\n",
                "        print(f\"   {split.capitalize():6s}: {data['n_images']:4d} imgs ({pct:.0f}%) | {data['avg_width']:.0f}x{data['avg_height']:.0f}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## üìä 10. Confronto Distribuzione Split"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# ============================================================================\n",
                "# VISUALIZZAZIONE CONFRONTO SPLIT\n",
                "# ============================================================================\n",
                "\n",
                "fig, axes = plt.subplots(1, 2, figsize=(12, 5))\n",
                "\n",
                "for ax, stats in zip(axes, [kaggle_stats, roboflow_stats]):\n",
                "    splits = list(stats['splits'].keys())\n",
                "    counts = [stats['splits'][s]['n_images'] for s in splits]\n",
                "    colors = ['#2ecc71', '#3498db', '#e74c3c']\n",
                "    \n",
                "    ax.pie(counts, labels=splits, autopct='%1.0f%%', colors=colors[:len(splits)],\n",
                "           explode=[0.02]*len(splits), shadow=True, startangle=90)\n",
                "    ax.set_title(f\"{stats['name']} Dataset\\n(Total: {sum(counts)} images)\", fontsize=12, fontweight='bold')\n",
                "\n",
                "plt.suptitle('üìä Distribuzione Split Dataset', fontsize=14, fontweight='bold', y=1.02)\n",
                "plt.tight_layout()\n",
                "plt.savefig(OUTPUT_DIR / 'eda_split_comparison.png', dpi=150, bbox_inches='tight')\n",
                "plt.show()"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## üñºÔ∏è 11. Visualizzazione Campioni"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# ============================================================================\n",
                "# VISUALIZZAZIONE CAMPIONI DA ENTRAMBI I DATASET\n",
                "# ============================================================================\n",
                "\n",
                "def show_samples(dataset_path: Path, title: str, n_samples=4):\n",
                "    \"\"\"Mostra campioni casuali da un dataset.\"\"\"\n",
                "    train_imgs = list((dataset_path / 'train' / 'images').glob('*.*'))\n",
                "    \n",
                "    if not train_imgs:\n",
                "        print(f\"‚ùå Nessuna immagine trovata in {dataset_path}\")\n",
                "        return\n",
                "    \n",
                "    samples = random.sample(train_imgs, min(n_samples, len(train_imgs)))\n",
                "    \n",
                "    fig, axes = plt.subplots(1, n_samples, figsize=(4*n_samples, 4))\n",
                "    if n_samples == 1:\n",
                "        axes = [axes]\n",
                "    \n",
                "    for ax, img_path in zip(axes, samples):\n",
                "        img = cv2.imread(str(img_path))\n",
                "        img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
                "        ax.imshow(img)\n",
                "        ax.set_title(img_path.name[:20], fontsize=9)\n",
                "        ax.axis('off')\n",
                "    \n",
                "    plt.suptitle(f'üñºÔ∏è {title}', fontsize=12, fontweight='bold')\n",
                "    plt.tight_layout()\n",
                "    plt.show()\n",
                "\n",
                "show_samples(DATASETS_DIR / 'kaggle_dataset', 'Campioni Dataset Kaggle')\n",
                "show_samples(DATASETS_DIR / 'roboflow_dataset', 'Campioni Dataset Roboflow')"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## üìè 12. Analisi Annotazioni"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# ============================================================================\n",
                "# ANALISI ANNOTAZIONI (NUMERO E DIMENSIONE POLIGONI)\n",
                "# ============================================================================\n",
                "\n",
                "def analyze_annotations(dataset_path: Path, name: str):\n",
                "    \"\"\"Analizza le annotazioni di un dataset.\"\"\"\n",
                "    label_dir = dataset_path / 'train' / 'labels'\n",
                "    \n",
                "    if not label_dir.exists():\n",
                "        return None\n",
                "    \n",
                "    polygon_counts = []\n",
                "    polygon_sizes = []\n",
                "    \n",
                "    for lbl_path in label_dir.glob('*.txt'):\n",
                "        with open(lbl_path, 'r') as f:\n",
                "            lines = f.readlines()\n",
                "        \n",
                "        polygon_counts.append(len(lines))\n",
                "        \n",
                "        for line in lines:\n",
                "            parts = line.strip().split()\n",
                "            if len(parts) > 1:\n",
                "                n_points = (len(parts) - 1) // 2\n",
                "                polygon_sizes.append(n_points)\n",
                "    \n",
                "    return {\n",
                "        'name': name,\n",
                "        'polygon_counts': polygon_counts,\n",
                "        'polygon_sizes': polygon_sizes,\n",
                "        'avg_polygons': np.mean(polygon_counts) if polygon_counts else 0,\n",
                "        'avg_points': np.mean(polygon_sizes) if polygon_sizes else 0\n",
                "    }\n",
                "\n",
                "kaggle_ann = analyze_annotations(DATASETS_DIR / 'kaggle_dataset', 'Kaggle')\n",
                "roboflow_ann = analyze_annotations(DATASETS_DIR / 'roboflow_dataset', 'Roboflow')\n",
                "\n",
                "# Visualizzazione\n",
                "fig, axes = plt.subplots(1, 2, figsize=(12, 4))\n",
                "\n",
                "for ax, ann in zip(axes, [kaggle_ann, roboflow_ann]):\n",
                "    if ann:\n",
                "        ax.hist(ann['polygon_counts'], bins=20, color='steelblue', edgecolor='black', alpha=0.7)\n",
                "        ax.axvline(ann['avg_polygons'], color='red', linestyle='--', linewidth=2, label=f\"Media: {ann['avg_polygons']:.1f}\")\n",
                "        ax.set_xlabel('Numero Banner per Immagine')\n",
                "        ax.set_ylabel('Frequenza')\n",
                "        ax.set_title(f\"{ann['name']} - Distribuzione Annotazioni\")\n",
                "        ax.legend()\n",
                "\n",
                "plt.suptitle('üìè Analisi Annotazioni', fontsize=14, fontweight='bold', y=1.02)\n",
                "plt.tight_layout()\n",
                "plt.savefig(OUTPUT_DIR / 'eda_annotations.png', dpi=150, bbox_inches='tight')\n",
                "plt.show()\n",
                "\n",
                "print(\"\\nüìè STATISTICHE ANNOTAZIONI\")\n",
                "print(\"=\"*60)\n",
                "for ann in [kaggle_ann, roboflow_ann]:\n",
                "    if ann:\n",
                "        print(f\"\\n{ann['name']}:\")\n",
                "        print(f\"   Media banner/img: {ann['avg_polygons']:.2f}\")\n",
                "        print(f\"   Media punti/polygon: {ann['avg_points']:.1f}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## üìã 13. Riepilogo Finale"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# ============================================================================\n",
                "# RIEPILOGO FINALE\n",
                "# ============================================================================\n",
                "\n",
                "print(\"=\"*60)\n",
                "print(\"‚úÖ EDA & DATA PREPARATION COMPLETATA\")\n",
                "print(\"=\"*60)\n",
                "print(\"\\nüìÅ Path dei dataset configurati:\")\n",
                "print(f\"   Kaggle YAML:   {KAG_YAML_PATH}\")\n",
                "print(f\"   Roboflow YAML: {ROBO_YAML_PATH}\")\n",
                "print(\"\\nüìä Grafici salvati in:\")\n",
                "print(f\"   {OUTPUT_DIR}\")\n",
                "print(\"\\nüöÄ Ora puoi procedere con il training notebook (SBS TRAINING.ipynb)\")\n",
                "print(\"=\"*60)"
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "Python 3",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbformat_minor": 2,
            "pygments_lexer": "ipython3",
            "version": "3.10.0"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 2
}