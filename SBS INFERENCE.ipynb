{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# ðŸŸï¸ Soccer Banner Segmentation - Inference Pipeline\n",
                "\n",
                "Questo notebook esegue l'inferenza sui modelli addestrati nella pipeline di training,\n",
                "applicando la segmentazione dei banner pubblicitari su video e immagini.\n",
                "\n",
                "## FunzionalitÃ  Principali\n",
                "1. **Selezione Modello**: Scelta facile tra le diverse versioni di modelli disponibili\n",
                "2. **Chroma Key Video**: Applicazione di green screen sui banner rilevati\n",
                "3. **Confronto Modelli**: Visualizzazione side-by-side Expert vs Specialist\n",
                "\n",
                "---"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## ðŸ“š 1. Import delle Librerie"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "import os\n",
                "import re\n",
                "from pathlib import Path\n",
                "\n",
                "import numpy as np\n",
                "import cv2\n",
                "import matplotlib.pyplot as plt\n",
                "\n",
                "import torch\n",
                "from ultralytics import YOLO  # type: ignore\n",
                "\n",
                "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
                "\n",
                "print(\"âœ… Ambiente di inferenza pronto.\")\n",
                "print(f\"   âš™ï¸  Dispositivo: {device.upper()}\")\n",
                "print(f\"   ðŸ”§ PyTorch: {torch.__version__}\")\n",
                "if torch.cuda.is_available():\n",
                "    print(f\"   ðŸŽ® GPU: {torch.cuda.get_device_name(0)}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## ðŸ“ 2. Configurazione Percorsi"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "BASE_DIR = Path(os.getcwd())\n",
                "INPUT_DIR = BASE_DIR / \"input\"\n",
                "OUTPUT_DIR = BASE_DIR / \"output\"\n",
                "MODEL_DIR = BASE_DIR / \"model\"\n",
                "\n",
                "VIDEO_OUT = OUTPUT_DIR / \"FINAL_VIDEOS\"\n",
                "VIDEO_OUT.mkdir(parents=True, exist_ok=True)\n",
                "\n",
                "VIDEO_IN = INPUT_DIR / 'video_input.mp4'\n",
                "\n",
                "print(\"âœ… Percorsi configurati:\")\n",
                "print(f\"   ðŸ“‚ Input:        {INPUT_DIR}\")\n",
                "print(f\"   ðŸ“‚ Output:       {OUTPUT_DIR}\")\n",
                "print(f\"   ðŸ“‚ Model:        {MODEL_DIR}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## ðŸ” 3. Funzioni per Selezione Modello"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "def list_available_models(model_dir: Path, model_type: str | None = None) -> list:\n",
                "    \"\"\"Elenca tutti i modelli disponibili.\"\"\"\n",
                "    models = []\n",
                "    if not model_dir.exists():\n",
                "        return models\n",
                "    \n",
                "    for item in sorted(model_dir.iterdir()):\n",
                "        if item.is_dir():\n",
                "            weights_path = item / 'best.pt'\n",
                "            if weights_path.exists():\n",
                "                if model_type is None or item.name.startswith(model_type):\n",
                "                    models.append((item.name, weights_path))\n",
                "    return models\n",
                "\n",
                "\n",
                "def get_latest_model(model_dir: Path, model_type: str) -> Path | None:\n",
                "    \"\"\"Ottiene l'ultima versione di un tipo di modello.\"\"\"\n",
                "    models = list_available_models(model_dir, model_type)\n",
                "    if not models:\n",
                "        return None\n",
                "    \n",
                "    def extract_version(name):\n",
                "        match = re.search(r'_v(\\d+)$', name)\n",
                "        return int(match.group(1)) if match else 0\n",
                "    \n",
                "    models.sort(key=lambda x: extract_version(x[0]), reverse=True)\n",
                "    return models[0][1]\n",
                "\n",
                "\n",
                "def display_available_models(model_dir: Path):\n",
                "    \"\"\"Mostra tutti i modelli disponibili.\"\"\"\n",
                "    print(\"\\n\" + \"=\"*60)\n",
                "    print(\"ðŸ“¦ MODELLI DISPONIBILI\")\n",
                "    print(\"=\"*60)\n",
                "    \n",
                "    expert_models = list_available_models(model_dir, 'expert_model')\n",
                "    specialist_models = list_available_models(model_dir, 'specialist_model')\n",
                "    \n",
                "    print(\"\\nðŸ‹ï¸ Expert Models:\")\n",
                "    for name, path in expert_models:\n",
                "        print(f\"   â€¢ {name}\")\n",
                "    if not expert_models:\n",
                "        print(\"   (nessuno)\")\n",
                "    \n",
                "    print(\"\\nðŸŽ¯ Specialist Models:\")\n",
                "    for name, path in specialist_models:\n",
                "        print(f\"   â€¢ {name}\")\n",
                "    if not specialist_models:\n",
                "        print(\"   (nessuno)\")\n",
                "    \n",
                "    print(\"\\n\" + \"=\"*60)\n",
                "\n",
                "\n",
                "display_available_models(MODEL_DIR)"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## âš™ï¸ 4. Selezione Versione Modello\n",
                "\n",
                "**Modifica le variabili per scegliere la versione:**"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# ============================================================================\n",
                "# SELEZIONE MODELLO\n",
                "# None = ultima versione, oppure 'v1', 'v2', etc.\n",
                "# ============================================================================\n",
                "\n",
                "EXPERT_VERSION = None\n",
                "SPECIALIST_VERSION = None\n",
                "\n",
                "# ============================================================================\n",
                "\n",
                "# Risolvi i path\n",
                "if EXPERT_VERSION is None:\n",
                "    EXPERT_WEIGHTS = get_latest_model(MODEL_DIR, 'expert_model')\n",
                "else:\n",
                "    EXPERT_WEIGHTS = MODEL_DIR / f'expert_model_{EXPERT_VERSION}' / 'best.pt'\n",
                "    if not EXPERT_WEIGHTS.exists():\n",
                "        EXPERT_WEIGHTS = None\n",
                "\n",
                "if SPECIALIST_VERSION is None:\n",
                "    SPECIALIST_WEIGHTS = get_latest_model(MODEL_DIR, 'specialist_model')\n",
                "else:\n",
                "    SPECIALIST_WEIGHTS = MODEL_DIR / f'specialist_model_{SPECIALIST_VERSION}' / 'best.pt'\n",
                "    if not SPECIALIST_WEIGHTS.exists():\n",
                "        SPECIALIST_WEIGHTS = None\n",
                "\n",
                "# Fallback a output/\n",
                "if EXPERT_WEIGHTS is None:\n",
                "    fallback = OUTPUT_DIR / 'expert_model' / 'weights' / 'best.pt'\n",
                "    if fallback.exists():\n",
                "        EXPERT_WEIGHTS = fallback\n",
                "\n",
                "if SPECIALIST_WEIGHTS is None:\n",
                "    fallback = OUTPUT_DIR / 'specialist_model' / 'weights' / 'best.pt'\n",
                "    if fallback.exists():\n",
                "        SPECIALIST_WEIGHTS = fallback\n",
                "\n",
                "# Estrai nomi versione per uso successivo\n",
                "EXPERT_VERSION_NAME = EXPERT_WEIGHTS.parent.name if EXPERT_WEIGHTS else 'N/A'\n",
                "SPECIALIST_VERSION_NAME = SPECIALIST_WEIGHTS.parent.name if SPECIALIST_WEIGHTS else 'N/A'\n",
                "\n",
                "print(\"\\nðŸ“¦ Modelli selezionati:\")\n",
                "print(f\"   {'âœ…' if EXPERT_WEIGHTS else 'âŒ'} Expert:     {EXPERT_VERSION_NAME}\")\n",
                "print(f\"   {'âœ…' if SPECIALIST_WEIGHTS else 'âŒ'} Specialist: {SPECIALIST_VERSION_NAME}\")\n",
                "print(f\"\\nðŸŽ¬ Video Input: {'âœ…' if VIDEO_IN.exists() else 'âŒ'} {VIDEO_IN.name}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## ðŸŽ¬ 5. Funzione Chroma Key"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "def apply_chroma_key(\n",
                "    model_path: Path,\n",
                "    video_path: Path,\n",
                "    output_name: str,\n",
                "    conf_threshold: float = 0.10,  # Ridotto standard a 0.10\n",
                "    multi_pass: bool = True\n",
                ") -> Path | None:\n",
                "    \"\"\"Applica chroma key sui banner rilevati.\"\"\"\n",
                "    \n",
                "    if model_path is None or not os.path.exists(model_path):\n",
                "        print(f\"âŒ Modello non trovato: {model_path}\")\n",
                "        return None\n",
                "    \n",
                "    if not os.path.exists(video_path):\n",
                "        print(f\"âŒ Video non trovato: {video_path}\")\n",
                "        return None\n",
                "    \n",
                "    print(f\"ðŸ“¦ Caricamento modello: {model_path.parent.name}\")\n",
                "    model = YOLO(str(model_path)).to(device)\n",
                "    \n",
                "    cap = cv2.VideoCapture(str(video_path))\n",
                "    width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
                "    height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
                "    fps = cap.get(cv2.CAP_PROP_FPS)\n",
                "    total_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n",
                "    \n",
                "    print(f\"ðŸŽ¬ Video: {width}x{height} @ {fps:.1f}fps ({total_frames} frames)\")\n",
                "    \n",
                "    out_file = VIDEO_OUT / output_name\n",
                "    fourcc = cv2.VideoWriter_fourcc(*'mp4v')  # type: ignore\n",
                "    writer = cv2.VideoWriter(str(out_file), fourcc, fps, (width, height))\n",
                "    \n",
                "    # KERNEL TUNING: \n",
                "    # 1. Close molto ampio (21x21) per riempire i buchi grandi\n",
                "    # 2. Dilate minimo (3x3 iter=1) per evitare di coprire la rete\n",
                "    kernel_dilate = np.ones((3, 3), np.uint8)\n",
                "    kernel_close = np.ones((21, 21), np.uint8)\n",
                "    \n",
                "    print(f\"â³ Elaborazione: {output_name}\")\n",
                "    frame_count = 0\n",
                "    \n",
                "    while cap.isOpened():\n",
                "        ret, frame = cap.read()\n",
                "        if not ret:\n",
                "            break\n",
                "        \n",
                "        if multi_pass:\n",
                "            # Confidenza piÃ¹ bassa per trovare parti mancanti\n",
                "            results_high = model.predict(frame, conf=conf_threshold * 0.8, imgsz=1280, device=device, verbose=False, retina_masks=True)\n",
                "            results_std = model.predict(frame, conf=conf_threshold * 0.6, imgsz=1024, device=device, verbose=False, retina_masks=True)\n",
                "            \n",
                "            masks_combined = []\n",
                "            if results_high[0].masks is not None:\n",
                "                masks_combined.append(results_high[0].masks.data)  # type: ignore\n",
                "            if results_std[0].masks is not None:\n",
                "                masks_combined.append(results_std[0].masks.data)  # type: ignore\n",
                "            \n",
                "            if masks_combined:\n",
                "                all_masks = torch.cat(masks_combined, dim=0)\n",
                "                combined = torch.any(all_masks, dim=0).int() * 255  # type: ignore\n",
                "            else:\n",
                "                combined = torch.zeros((height, width), dtype=torch.int32)\n",
                "        else:\n",
                "            results = model.predict(frame, conf=conf_threshold, imgsz=1280, device=device, verbose=False, retina_masks=True)\n",
                "            if results[0].masks is not None:\n",
                "                combined = torch.any(results[0].masks.data, dim=0).int() * 255  # type: ignore\n",
                "            else:\n",
                "                combined = torch.zeros((height, width), dtype=torch.int32)\n",
                "        \n",
                "        mask_np = combined.cpu().numpy().astype(np.uint8)  # type: ignore\n",
                "        mask_final = cv2.resize(mask_np, (width, height), interpolation=cv2.INTER_CUBIC)\n",
                "        \n",
                "        # POST-PROCESSING\n",
                "        # 1. Chiudere i buchi (aggressive)\n",
                "        mask_final = cv2.morphologyEx(mask_final, cv2.MORPH_CLOSE, kernel_close)\n",
                "        # 2. Dilatazione minima\n",
                "        mask_final = cv2.dilate(mask_final, kernel_dilate, iterations=1)\n",
                "        # 3. Blur\n",
                "        mask_final = cv2.GaussianBlur(mask_final, (5, 5), 0)\n",
                "        \n",
                "        frame[mask_final > 100] = [0, 255, 0]\n",
                "        writer.write(frame)\n",
                "        frame_count += 1\n",
                "        \n",
                "        if frame_count % 50 == 0:\n",
                "            print(f\"   ðŸ“Š {frame_count}/{total_frames} ({frame_count/total_frames*100:.1f}%)\")\n",
                "    \n",
                "    cap.release()\n",
                "    writer.release()\n",
                "    \n",
                "    print(f\"âœ… Completato: {out_file}\")\n",
                "    print(f\"   ðŸ“ Size: {out_file.stat().st_size / (1024*1024):.1f} MB\")\n",
                "    return out_file"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## ðŸ‹ï¸ 6. Inferenza con Modello Expert"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "if EXPERT_WEIGHTS is not None:\n",
                "    print(\"=\"*60)\n",
                "    print(f\"ðŸ‹ï¸ INFERENZA CON MODELLO EXPERT ({EXPERT_VERSION_NAME})\")\n",
                "    print(\"=\"*60)\n",
                "    \n",
                "    expert_output = apply_chroma_key(\n",
                "        model_path=EXPERT_WEIGHTS,\n",
                "        video_path=VIDEO_IN,\n",
                "        output_name=f'video_{EXPERT_VERSION_NAME}.mp4'\n",
                "    )\n",
                "else:\n",
                "    print(\"âš ï¸  Modello Expert non trovato.\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## ðŸŽ¯ 7. Inferenza con Modello Specialist"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "if SPECIALIST_WEIGHTS is not None:\n",
                "    print(\"=\"*60)\n",
                "    print(f\"ðŸŽ¯ INFERENZA CON MODELLO SPECIALIST ({SPECIALIST_VERSION_NAME})\")\n",
                "    print(\"=\"*60)\n",
                "    \n",
                "    specialist_output = apply_chroma_key(\n",
                "        model_path=SPECIALIST_WEIGHTS,\n",
                "        video_path=VIDEO_IN,\n",
                "        output_name=f'video_{SPECIALIST_VERSION_NAME}.mp4'\n",
                "    )\n",
                "else:\n",
                "    print(\"âš ï¸  Modello Specialist non trovato.\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## ðŸ“Š 8. Confronto Visivo dei Modelli"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "def compare_models_on_frames(video_path, expert_weights, specialist_weights, expert_name, specialist_name, num_frames=4):\n",
                "    \"\"\"Confronta Expert vs Specialist su frame estratti.\"\"\"\n",
                "    \n",
                "    if expert_weights is None or specialist_weights is None:\n",
                "        print(\"âŒ Entrambi i modelli sono necessari per il confronto.\")\n",
                "        return\n",
                "    \n",
                "    if not video_path.exists():\n",
                "        print(f\"âŒ Video non trovato: {video_path}\")\n",
                "        return\n",
                "    \n",
                "    print(\"=\"*60)\n",
                "    print(f\"ðŸ“Š CONFRONTO: {expert_name} vs {specialist_name}\")\n",
                "    print(\"=\"*60)\n",
                "    \n",
                "    cap = cv2.VideoCapture(str(video_path))\n",
                "    total_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n",
                "    frame_indices = [int(i * total_frames / (num_frames + 1)) for i in range(1, num_frames + 1)]\n",
                "    \n",
                "    print(\"\\nðŸ“¦ Caricamento modelli...\")\n",
                "    expert_model = YOLO(str(expert_weights)).to(device)\n",
                "    specialist_model = YOLO(str(specialist_weights)).to(device)\n",
                "    print(f\"   Expert: {expert_name}\")\n",
                "    print(f\"   Specialist: {specialist_name}\")\n",
                "    \n",
                "    fig, axes = plt.subplots(num_frames, 3, figsize=(15, 4*num_frames))\n",
                "    if num_frames == 1:\n",
                "        axes = [axes]\n",
                "    \n",
                "    for idx, frame_idx in enumerate(frame_indices):\n",
                "        cap.set(cv2.CAP_PROP_POS_FRAMES, frame_idx)\n",
                "        ret, frame = cap.read()\n",
                "        if not ret:\n",
                "            continue\n",
                "        \n",
                "        frame_rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
                "        \n",
                "        # Expert\n",
                "        expert_result = expert_model.predict(frame, conf=0.15, device=device, verbose=False)\n",
                "        expert_frame = frame.copy()\n",
                "        if expert_result[0].masks is not None:\n",
                "            mask = expert_result[0].masks.data.cpu().numpy()  # type: ignore\n",
                "            combined_mask = np.any(mask, axis=0).astype(np.uint8) * 255\n",
                "            combined_mask = cv2.resize(combined_mask, (frame.shape[1], frame.shape[0]))\n",
                "            expert_frame[combined_mask > 100] = [0, 255, 0]\n",
                "        \n",
                "        # Specialist\n",
                "        specialist_result = specialist_model.predict(frame, conf=0.15, device=device, verbose=False)\n",
                "        specialist_frame = frame.copy()\n",
                "        if specialist_result[0].masks is not None:\n",
                "            mask = specialist_result[0].masks.data.cpu().numpy()  # type: ignore\n",
                "            combined_mask = np.any(mask, axis=0).astype(np.uint8) * 255\n",
                "            combined_mask = cv2.resize(combined_mask, (frame.shape[1], frame.shape[0]))\n",
                "            specialist_frame[combined_mask > 100] = [0, 255, 0]\n",
                "        \n",
                "        # Plot con versione nei titoli\n",
                "        axes[idx][0].imshow(frame_rgb)\n",
                "        axes[idx][0].set_title(f'Originale (Frame {frame_idx})')\n",
                "        axes[idx][0].axis('off')\n",
                "        \n",
                "        axes[idx][1].imshow(cv2.cvtColor(expert_frame, cv2.COLOR_BGR2RGB))\n",
                "        axes[idx][1].set_title(f'Expert ({expert_name})')\n",
                "        axes[idx][1].axis('off')\n",
                "        \n",
                "        axes[idx][2].imshow(cv2.cvtColor(specialist_frame, cv2.COLOR_BGR2RGB))\n",
                "        axes[idx][2].set_title(f'Specialist ({specialist_name})')\n",
                "        axes[idx][2].axis('off')\n",
                "    \n",
                "    cap.release()\n",
                "    plt.tight_layout()\n",
                "    # Salvataggio confronto\n",
                "    confronti_dir = OUTPUT_DIR / 'confronti'\n",
                "    confronti_dir.mkdir(parents=True, exist_ok=True)\n",
                "    base_name = 'confronto'\n",
                "    file_name = f'{base_name}.png'\n",
                "    counter = 1\n",
                "    while (confronti_dir / file_name).exists():\n",
                "        file_name = f'{base_name}_{counter}.png'\n",
                "        counter += 1\n",
                "    save_path = confronti_dir / file_name\n",
                "    plt.savefig(save_path)\n",
                "    print(f'\\n   ðŸ’¾ Immagine salvata in: {save_path}')\n",
                "    plt.show()\n",
                "    print(\"\\nâœ… Confronto completato.\")\n",
                "\n",
                "# Esegui confronto\n",
                "if EXPERT_WEIGHTS and SPECIALIST_WEIGHTS and VIDEO_IN.exists():\n",
                "    compare_models_on_frames(\n",
                "        VIDEO_IN, \n",
                "        EXPERT_WEIGHTS, \n",
                "        SPECIALIST_WEIGHTS,\n",
                "        EXPERT_VERSION_NAME,\n",
                "        SPECIALIST_VERSION_NAME,\n",
                "        num_frames=4\n",
                "    )"
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "Python 3",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbformat_minor": 2,
            "pygments_lexer": "ipython3",
            "version": "3.10.0"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 2
}
